{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d137f39-2765-4c33-a3b2-4adb2e31c388",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#importing libraries\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import string \n",
    "import re\n",
    "import nltk\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "[nltk_data] Downloading package stopwords to\n",
    "[nltk_data]     C:\\Users\\VIKRAM\\AppData\\Roaming\\nltk_data...\n",
    "[nltk_data]   Unzipping corpora\\stopwords.zip.\n",
    "[nltk_data] Downloading package wordnet to\n",
    "[nltk_data]     C:\\Users\\VIKRAM\\AppData\\Roaming\\nltk_data...\n",
    "True\n",
    "#reading the dataset \n",
    "#dataset: https://www.kaggle.com/uciml/sms-spam-collection-data\n",
    "msg=pd.read_csv('C:/New folder/spam_detector.csv',encoding='latin-1')\n",
    "msg.drop(['Unnamed: 2','Unnamed: 3','Unnamed: 4'],axis=1,inplace=True)\n",
    "msg.rename(columns={'v1':'label','v2':'text'},inplace=True)\n",
    "msg.head()\n",
    "label\ttext\n",
    "0\tham\tGo until jurong point, crazy.. Available only ...\n",
    "1\tham\tOk lar... Joking wif u oni...\n",
    "2\tspam\tFree entry in 2 a wkly comp to win FA Cup fina...\n",
    "3\tham\tU dun say so early hor... U c already then say...\n",
    "4\tham\tNah I don't think he goes to usf, he lives aro...\n",
    "#mapping ham=0 and spam=1\n",
    "for i in msg.index:\n",
    "  if msg['label'][i]=='ham':\n",
    "    msg['label'][i]=0\n",
    "  else:\n",
    "    msg['label'][i]=1\n",
    "msg.head()\n",
    "label\ttext\n",
    "0\t0\tGo until jurong point, crazy.. Available only ...\n",
    "1\t0\tOk lar... Joking wif u oni...\n",
    "2\t1\tFree entry in 2 a wkly comp to win FA Cup fina...\n",
    "3\t0\tU dun say so early hor... U c already then say...\n",
    "4\t0\tNah I don't think he goes to usf, he lives aro...\n",
    "#category count plot (count of spam and ham)\n",
    "sns.countplot(msg.label)\n",
    "<Axes: ylabel='count'>\n",
    "\n",
    "#data description grouped by labels \n",
    "msg.groupby('label').describe()\n",
    "text\n",
    "count\tunique\ttop\tfreq\n",
    "label\t\t\t\t\n",
    "0\t4825\t4516\tSorry, I'll call later\t30\n",
    "1\t747\t653\tPlease call our customer service representativ...\t4\n",
    "#dropping duplicate rows\n",
    "msg=msg.drop_duplicates()\n",
    "msg.groupby('label').describe()\n",
    "text\n",
    "count\tunique\ttop\tfreq\n",
    "label\t\t\t\t\n",
    "0\t4516\t4516\tGo until jurong point, crazy.. Available only ...\t1\n",
    "1\t653\t653\tFree entry in 2 a wkly comp to win FA Cup fina...\t1\n",
    "#adding length column to the dataset \n",
    "msg['length']=msg['text'].apply(len)\n",
    "msg.head()\n",
    "label\ttext\tlength\n",
    "0\t0\tGo until jurong point, crazy.. Available only ...\t111\n",
    "1\t0\tOk lar... Joking wif u oni...\t29\n",
    "2\t1\tFree entry in 2 a wkly comp to win FA Cup fina...\t155\n",
    "3\t0\tU dun say so early hor... U c already then say...\t49\n",
    "4\t0\tNah I don't think he goes to usf, he lives aro...\t61\n",
    "msg[msg.label==0].describe()\n",
    "length\n",
    "count\t4516.000000\n",
    "mean\t70.459256\n",
    "std\t56.358207\n",
    "min\t2.000000\n",
    "25%\t34.000000\n",
    "50%\t52.000000\n",
    "75%\t90.000000\n",
    "max\t910.000000\n",
    "sns.distplot(a=msg[msg['label']==0].length,kde=False)\n",
    "<Axes: xlabel='length'>\n",
    "\n",
    "msg[msg.label==1].describe()\n",
    "length\n",
    "count\t653.000000\n",
    "mean\t137.891271\n",
    "std\t30.137753\n",
    "min\t13.000000\n",
    "25%\t132.000000\n",
    "50%\t149.000000\n",
    "75%\t157.000000\n",
    "max\t224.000000\n",
    "sns.distplot(a=msg[msg['label']==1].length,kde=False)\n",
    "<Axes: xlabel='length'>\n",
    "\n",
    "#examining spam texts\n",
    "for i in range(50):\n",
    "  if msg['label'][i]==1:\n",
    "    print(msg['text'][i])\n",
    "Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&C's apply 08452810075over18's\n",
    "FreeMsg Hey there darling it's been 3 week's now and no word back! I'd like some fun you up for it still? Tb ok! XxX std chgs to send, å£1.50 to rcv\n",
    "WINNER!! As a valued network customer you have been selected to receivea å£900 prize reward! To claim call 09061701461. Claim code KL341. Valid 12 hours only.\n",
    "Had your mobile 11 months or more? U R entitled to Update to the latest colour mobiles with camera for Free! Call The Mobile Update Co FREE on 08002986030\n",
    "SIX chances to win CASH! From 100 to 20,000 pounds txt> CSH11 and send to 87575. Cost 150p/day, 6days, 16+ TsandCs apply Reply HL 4 info\n",
    "URGENT! You have won a 1 week FREE membership in our å£100,000 Prize Jackpot! Txt the word: CLAIM to No: 81010 T&C www.dbuk.net LCCLTD POBOX 4403LDNW1A7RW18\n",
    "XXXMobileMovieClub: To use your credit, click the WAP link in the next txt message or click here>> http://wap. xxxmobilemovieclub.com?n=QJKGIGHJJGCBL\n",
    "England v Macedonia - dont miss the goals/team news. Txt ur national team to 87077 eg ENGLAND to 87077 Try:WALES, SCOTLAND 4txt/Ì¼1.20 POBOXox36504W45WQ 16+\n",
    "Thanks for your subscription to Ringtone UK your mobile will be charged å£5/month Please confirm by replying YES or NO. If you reply NO you will not be charged\n",
    "07732584351 - Rodger Burns - MSG = We tried to call you re your reply to our sms for a free nokia mobile + free camcorder. Please call now 08000930705 for delivery tomorrow\n",
    "msg['contain']=msg['text'].str.contains('£').map({False:0,True:1})\n",
    "msg['contain']=msg['contain']|msg['text'].str.contains('%').map({False:0,True:1})\n",
    "msg['contain']=msg['contain']|msg['text'].str.contains('€').map({False:0,True:1})\n",
    "msg['contain']=msg['contain']|msg['text'].str.contains('\\$').map({False:0,True:1})\n",
    "msg['contain']=msg['contain']|msg['text'].str.contains(\"T&C\").map({False:0,True:1})\n",
    "msg['contain']=msg['contain']|msg['text'].str.contains(\"www|WWW\").map({False:0,True:1})\n",
    "msg['contain']=msg['contain']|msg['text'].str.contains(\"http|HTTP\").map({False:0,True:1})\n",
    "msg['contain']=msg['contain']|msg['text'].str.contains(\"https|HTTPS\").map({False:0,True:1})\n",
    "msg['contain']=msg['contain']|msg['text'].str.contains(\"@\").map({False:0,True:1})\n",
    "msg['contain']=msg['contain']|msg['text'].str.contains(\"email|Email|EMAIL\").map({False:0,True:1})\n",
    "msg['contain']=msg['contain']|msg['text'].str.contains(\"SMS|sms|FREEPHONE\").map({False:0,True:1})\n",
    "msg['contain']=msg['contain']|msg['text'].str.contains(\"\\d{11}\",regex=True).map({False:0,True:1})\n",
    "msg['contain']=msg['contain']|msg['text'].str.contains(\"\\d{10}\",regex=True).map({False:0,True:1})\n",
    "msg['contain']=msg['contain']|msg['text'].str.contains(\"\\d{5}\",regex=True).map({False:0,True:1})\n",
    "\n",
    "msg.head()\n",
    "label\ttext\tlength\tcontain\n",
    "0\t0\tGo until jurong point, crazy.. Available only ...\t111\t0\n",
    "1\t0\tOk lar... Joking wif u oni...\t29\t0\n",
    "2\t1\tFree entry in 2 a wkly comp to win FA Cup fina...\t155\t1\n",
    "3\t0\tU dun say so early hor... U c already then say...\t49\t0\n",
    "4\t0\tNah I don't think he goes to usf, he lives aro...\t61\t0\n",
    "sns.distplot(a=msg[msg['label']==0].contain,kde=False)\n",
    "<Axes: xlabel='contain'>\n",
    "\n",
    "sns.distplot(a=msg[msg['label']==1].contain,kde=False)\n",
    "<Axes: xlabel='contain'>\n",
    "\n",
    "#data cleaning/preprocessing - removing punctuation and digits \n",
    "msg['cleaned_text']=\"\"\n",
    "\n",
    "for i in msg.index:\n",
    "  updated_list=[]\n",
    "  for j in range(len(msg['text'][i])):\n",
    "    if msg['text'][i][j] not in string.punctuation:\n",
    "      if msg['text'][i][j].isdigit()==False:\n",
    "        updated_list.append(msg['text'][i][j])\n",
    "  updated_string=\"\".join(updated_list)\n",
    "  msg['cleaned_text'][i]=updated_string\n",
    "\n",
    "msg.drop(['text'],axis=1,inplace=True)\n",
    "msg.head() \n",
    "label\tlength\tcontain\tcleaned_text\n",
    "0\t0\t111\t0\tGo until jurong point crazy Available only in ...\n",
    "1\t0\t29\t0\tOk lar Joking wif u oni\n",
    "2\t1\t155\t1\tFree entry in a wkly comp to win FA Cup final...\n",
    "3\t0\t49\t0\tU dun say so early hor U c already then say\n",
    "4\t0\t61\t0\tNah I dont think he goes to usf he lives aroun...\n",
    "#data cleaning/preprocessing - tokenization and convert to lower case \n",
    "msg['token']=\"\"\n",
    "\n",
    "for i in msg.index:\n",
    "  msg['token'][i]=re.split(\"\\W+\",msg['cleaned_text'][i].lower())\n",
    "\n",
    "msg.head()\n",
    "label\tlength\tcontain\tcleaned_text\ttoken\n",
    "0\t0\t111\t0\tGo until jurong point crazy Available only in ...\t[go, until, jurong, point, crazy, available, o...\n",
    "1\t0\t29\t0\tOk lar Joking wif u oni\t[ok, lar, joking, wif, u, oni]\n",
    "2\t1\t155\t1\tFree entry in a wkly comp to win FA Cup final...\t[free, entry, in, a, wkly, comp, to, win, fa, ...\n",
    "3\t0\t49\t0\tU dun say so early hor U c already then say\t[u, dun, say, so, early, hor, u, c, already, t...\n",
    "4\t0\t61\t0\tNah I dont think he goes to usf he lives aroun...\t[nah, i, dont, think, he, goes, to, usf, he, l...\n",
    "#data cleaning/preprocessing - stopwords\n",
    "msg['updated_token']=\"\"\n",
    "stopwords=nltk.corpus.stopwords.words('english')\n",
    "\n",
    "for i in msg.index:\n",
    "  updated_list=[]\n",
    "  for j in range(len(msg['token'][i])):\n",
    "    if msg['token'][i][j] not in stopwords:\n",
    "      updated_list.append(msg['token'][i][j])\n",
    "  msg['updated_token'][i]=updated_list\n",
    "\n",
    "msg.drop(['token'],axis=1,inplace=True)\n",
    "msg.head()\n",
    "label\tlength\tcontain\tcleaned_text\tupdated_token\n",
    "0\t0\t111\t0\tGo until jurong point crazy Available only in ...\t[go, jurong, point, crazy, available, bugis, n...\n",
    "1\t0\t29\t0\tOk lar Joking wif u oni\t[ok, lar, joking, wif, u, oni]\n",
    "2\t1\t155\t1\tFree entry in a wkly comp to win FA Cup final...\t[free, entry, wkly, comp, win, fa, cup, final,...\n",
    "3\t0\t49\t0\tU dun say so early hor U c already then say\t[u, dun, say, early, hor, u, c, already, say]\n",
    "4\t0\t61\t0\tNah I dont think he goes to usf he lives aroun...\t[nah, dont, think, goes, usf, lives, around, t...\n",
    "#data cleaning/preprocessing - lemmatization \n",
    "msg['lem_text']=\"\"\n",
    "wordlem=nltk.WordNetLemmatizer()\n",
    "\n",
    "for i in msg.index:\n",
    "  updated_list=[]\n",
    "  for j in range(len(msg['updated_token'][i])):\n",
    "    updated_list.append(wordlem.lemmatize(msg['updated_token'][i][j]))\n",
    "  msg['lem_text'][i]=updated_list \n",
    "\n",
    "msg.drop(['updated_token'],axis=1,inplace=True)\n",
    "msg.head()\n",
    "label\tlength\tcontain\tcleaned_text\tlem_text\n",
    "0\t0\t111\t0\tGo until jurong point crazy Available only in ...\t[go, jurong, point, crazy, available, bugis, n...\n",
    "1\t0\t29\t0\tOk lar Joking wif u oni\t[ok, lar, joking, wif, u, oni]\n",
    "2\t1\t155\t1\tFree entry in a wkly comp to win FA Cup final...\t[free, entry, wkly, comp, win, fa, cup, final,...\n",
    "3\t0\t49\t0\tU dun say so early hor U c already then say\t[u, dun, say, early, hor, u, c, already, say]\n",
    "4\t0\t61\t0\tNah I dont think he goes to usf he lives aroun...\t[nah, dont, think, go, usf, life, around, though]\n",
    "#data cleaning/preprocessing - merging token\n",
    "msg['final_text']=\"\"\n",
    "\n",
    "for i in msg.index:\n",
    "  updated_string=\" \".join(msg['lem_text'][i])\n",
    "  msg['final_text'][i]=updated_string\n",
    "\n",
    "msg.drop(['cleaned_text','lem_text'],axis=1,inplace=True)\n",
    "msg.head()\n",
    "label\tlength\tcontain\tfinal_text\n",
    "0\t0\t111\t0\tgo jurong point crazy available bugis n great ...\n",
    "1\t0\t29\t0\tok lar joking wif u oni\n",
    "2\t1\t155\t1\tfree entry wkly comp win fa cup final tkts st ...\n",
    "3\t0\t49\t0\tu dun say early hor u c already say\n",
    "4\t0\t61\t0\tnah dont think go usf life around though\n",
    "#separating target and features\n",
    "y=pd.DataFrame(msg.label)\n",
    "x=msg.drop(['label'],axis=1)\n",
    "#splitting the data (80:20 ratio)\n",
    "x_train,x_val,y_train,y_val=train_test_split(x,y,train_size=0.8,test_size=0.2,random_state=0)\n",
    "#count vectorization \n",
    "cv=CountVectorizer(max_features=5000)\n",
    "temp_train=cv.fit_transform(x_train['final_text']).toarray()\n",
    "temp_val=cv.transform(x_val['final_text']).toarray()\n",
    "#tfidf\n",
    "tf=TfidfTransformer()\n",
    "temp_train=tf.fit_transform(temp_train)\n",
    "temp_val=tf.transform(temp_val)\n",
    "#merging temp datafram with original dataframe\n",
    "temp_train=pd.DataFrame(temp_train.toarray(),index=x_train.index)\n",
    "temp_val=pd.DataFrame(temp_val.toarray(),index=x_val.index)\n",
    "x_train=pd.concat([x_train,temp_train],axis=1,sort=False)\n",
    "x_val=pd.concat([x_val,temp_val],axis=1,sort=False)\n",
    "\n",
    "x_train.head()\n",
    "length\tcontain\tfinal_text\t0\t1\t2\t3\t4\t5\t6\t...\t4990\t4991\t4992\t4993\t4994\t4995\t4996\t4997\t4998\t4999\n",
    "3794\t45\t0\talso remember bead dont come ever\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t...\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\n",
    "4290\t25\t0\tcall ure done\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t...\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\n",
    "2603\t99\t0\tim arestaurant eating squid wanna dosomething ...\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t...\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\n",
    "3452\t274\t0\tnowadays people notixiquating laxinorficated o...\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t...\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\n",
    "3132\t25\t0\tmessage food\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t...\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\n",
    "5 rows × 5003 columns\n",
    "\n",
    "#dropping the final_text column\n",
    "x_train.drop(['final_text'],axis=1,inplace=True)\n",
    "x_val.drop(['final_text'],axis=1,inplace=True)\n",
    "\n",
    "x_train.head()\n",
    "length\tcontain\t0\t1\t2\t3\t4\t5\t6\t7\t...\t4990\t4991\t4992\t4993\t4994\t4995\t4996\t4997\t4998\t4999\n",
    "3794\t45\t0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t...\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\n",
    "4290\t25\t0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t...\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\n",
    "2603\t99\t0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t...\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\n",
    "3452\t274\t0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t...\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\n",
    "3132\t25\t0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t...\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\n",
    "5 rows × 5002 columns\n",
    "\n",
    "#converting the labels to int datatype (for model training)\n",
    "y_train=y_train.astype(int)\n",
    "y_val=y_val.astype(int)\n",
    "#Multinomial Naive Bayes\n",
    "model=MultinomialNB()\n",
    "(x_train,y_train)\n",
    "\n",
    "print(\"Multinomial Naive Bayes:\",accuracy_score)\n",
    "Multinomial Naive Bayes: <function accuracy_score at 0x00000230EF1544A0>\n",
    "#Decision Tree\n",
    "model=DecisionTreeClassifier(random_state=0)\n",
    "(x_train,y_train)\n",
    "\n",
    "print(\"Decision Tree:\",accuracy_score)\n",
    "Decision Tree: <function accuracy_score at 0x00000230EF1544A0>\n",
    "#Random Forest\n",
    "model=RandomForestClassifier(n_estimators=100,random_state=0)\n",
    "\n",
    "\n",
    "print(\"Random Forest:\",accuracy_score)\n",
    "Random Forest: <function accuracy_score at 0x00000230EF1544A0>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
